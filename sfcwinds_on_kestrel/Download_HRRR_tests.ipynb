{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3740c070-16ae-4171-ac13-60d42ab5d185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nHRRR model documentation:\\nhttps://rapidrefresh.noaa.gov/hrrr/\\n\\nDocumentation for the HRRR data download wrapper:\\nhttps://herbie.readthedocs.io/en/stable/\\n\\nVariables:\\nhttps://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf00.grib2.shtml\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from herbie import Herbie\n",
    "import herbie\n",
    "#from toolbox import EasyMap, pc\n",
    "#from paint.standard2 import cm_tmp\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import shapely.geometry as sgeom\n",
    "import cartopy.feature\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from cartopy.io.shapereader import Reader\n",
    "import numpy as np\n",
    "#from toolbox.cartopy_tools import common_features, pc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.cm as cm\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Will not remove GRIB file because it previously existed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "HRRR model documentation:\n",
    "https://rapidrefresh.noaa.gov/hrrr/\n",
    "\n",
    "Documentation for the HRRR data download wrapper:\n",
    "https://herbie.readthedocs.io/en/stable/\n",
    "\n",
    "Variables:\n",
    "https://www.nco.ncep.noaa.gov/pmb/products/hrrr/hrrr.t00z.wrfnatf00.grib2.shtml\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3147138c-7e6a-4a47-b8ba-af1d3b906d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define\n",
    "\n",
    "# Folder to save datasets\n",
    "save_folder = \"/kfs2/projects/sfcwinds/HRRR\"\n",
    "# save_folder = \"../data/HRRR\"\n",
    "\n",
    "# Define download area\n",
    "\n",
    "# # Have not found a way to slice by lat and lon directly, use x,y instead (below)\n",
    "# min_lat = 35\n",
    "# max_lat = 38\n",
    "# min_lon = -111\n",
    "# max_lon = -118\n",
    "\n",
    "\n",
    "# Define area to download (boundaries of download area)\n",
    "area = \"US_SW\"   #   \"NM_AZ\", \"US\", \"US_SW\"\n",
    "\n",
    "if area == \"NM_AZ\":   # New Mexico and Arizona\n",
    "    slicex = slice(300, 800)\n",
    "    slicey = slice(300, 600)\n",
    "\n",
    "elif area == \"US\":  # Entire continental US\n",
    "    slicex = slice(130, 1730)\n",
    "    slicey = slice(50, 1040)       \n",
    "\n",
    "elif area == \"US_SW\":  # US Southwest\n",
    "    slicex = slice(130, 1000)\n",
    "    slicey = slice(50, 800)    \n",
    "\n",
    "\n",
    "# date range with one-day frequency\n",
    "date_range = pd.date_range(datetime(2014, 1, 1), \n",
    "                           datetime(2024, 7, 31), \n",
    "                           freq=\"D\").tolist()[::-1]   # list starts from end\n",
    "\n",
    "# Forecast hour (0=analysis, 2 is recommended)\n",
    "fxx = 2    \n",
    "\n",
    "# Chunk dictionaries\n",
    "chunk_dict_hourly = {\"valid_time\": 1, \"isobaricInhPa\": 1, \"y\": 200, \"x\": 200}\n",
    "chunk_dict_daily = {\"valid_time\": 12, \"isobaricInhPa\": 1, \"y\": 200, \"x\": 200}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define compression settings\n",
    "comp = {\"zlib\": True, \"complevel\": 4}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce1a20b-0685-4b24-b893-3222dcafcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Functions\n",
    "\n",
    "\n",
    "def safe_xarray(herbie_obj, search_str):\n",
    "    \n",
    "    \"\"\" accessing the Herbie object sometimes gives a permission error, adding a wait time helps.\"\"\"\n",
    "    \n",
    "    for _ in range(5):  # Retry up to 5 times\n",
    "        try:\n",
    "            return herbie_obj.xarray(search_str, remove_grib=True)\n",
    "        except PermissionError:\n",
    "            time.sleep(1)  # Wait and retry\n",
    "    raise Exception(f\"Failed to load {search_str} after multiple attempts.\")\n",
    "    \n",
    "    \n",
    "def process_hourly_data(timestamp, area, slicey, slicex, fxx):\n",
    "    \"\"\"Retrieve, process, and return hourly HRRR dataset\"\"\"\n",
    "    \n",
    "    max_retries = 10  # Set a limit to avoid infinite loops\n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_retries:\n",
    "        \n",
    "        try:\n",
    "            valid_time = timestamp + pd.to_timedelta(fxx, \"h\") # 2-hour shift (forecast lead time fxx)\n",
    "    \n",
    "            print(f\"{valid_time}\")\n",
    "    \n",
    "            H_sfc = Herbie(timestamp, model='hrrr', product='sfc', fxx=fxx, verbose=False)\n",
    "            H_subh = Herbie(timestamp, model='hrrr', product='subh', fxx=fxx, verbose=False)\n",
    "            \n",
    "            #H_sfc.inventory().variable.values   \n",
    "            #H_sfc.inventory()[[\"variable\", \"search_this\"]].values \n",
    "    \n",
    "            # Define search patterns for variables\n",
    "            # df = H_sfc.inventory()[[\"variable\", \"search_this\"]]\n",
    "            # filtered_df = df[df[\"search_this\"].str.contains(r\"\\b(?:ugrd|vgrd)\\b\", case=False, na=False) & \n",
    "            #                  df[\"search_this\"].str.contains(\"mb\", case=False, na=False)]\n",
    "            # wind_search_string_sfc = \"|\".join(filtered_df[\"search_this\"].drop_duplicates().values)\n",
    "            wind_search_string_sfc = ':UGRD:250 mb:2 hour fcst|:VGRD:250 mb:2 hour fcst|:UGRD:500 mb:2 hour fcst|:VGRD:500 mb:2 hour fcst|:UGRD:850 mb:2 hour fcst|:VGRD:850 mb:2 hour fcst'\n",
    "\n",
    "            \n",
    "            \n",
    "            df = H_subh.inventory()[[\"variable\", \"search_this\"]]\n",
    "            filtered_df = df[df[\"search_this\"].str.contains(r\"\\b(?:ugrd|vgrd)\\b\", case=False, na=False) & \n",
    "                             df[\"search_this\"].str.contains(\"ave\", case=False, na=False)]\n",
    "            wind_search_string_subh = \"|\".join(filtered_df[\"search_this\"].drop_duplicates().values)\n",
    "                    \n",
    "    \n",
    "            \n",
    "            # Surface data\n",
    "            ds_sfc = xr.merge([\n",
    "                safe_xarray(H_sfc, wind_search_string_sfc),\n",
    "                safe_xarray(H_sfc, ':TMP:2 m'),\n",
    "                safe_xarray(H_sfc, 'GUST|:PRES:surface:2 hour fcst|:SNOWC:surface:2 hour fcst|:APCP:surface:0-2 hour acc fcst|:SFCR:surface:2 hour fcst|:VEG:surface:2 hour fcst|:VGTYP:surface:2 hour fcst|:HPBL:surface:2 hour fcst'),\n",
    "                safe_xarray(H_sfc, '[U|V]GRD:10 m'),\n",
    "                safe_xarray(H_sfc, '[U|V]GRD:80 m').rename({\"u\": \"u80\", \"v\": \"v80\"}),\n",
    "                safe_xarray(H_sfc, ':DPT:2 m above ground:2 hour fcst'),\n",
    "                safe_xarray(H_sfc, ':TCDC:boundary layer cloud layer:2 hour fcst'),\n",
    "                safe_xarray(H_sfc, ':CAPE:0-3000 m above ground:2 hour fcst')\n",
    "            ], compat='minimal')\n",
    "    \n",
    "            ds_sfc = ds_sfc.sel(y=slicey, x=slicex).expand_dims(dim=\"valid_time\")\n",
    "            \n",
    "            # ds_sfc['wspd10'] = (ds_sfc.u10**2 + ds_sfc.v10**2)**0.5\n",
    "            # ds_sfc['wdir10'] = np.degrees(np.arctan2(ds_sfc.u10, ds_sfc.v10)) +180\n",
    "            \n",
    "            ds_sfc = ds_sfc.rename({\"u10\": \"u10_h\",\n",
    "                                    \"v10\": \"v10_h\",\n",
    "                                    # \"wspd10\": \"wspd10_h\",\n",
    "                                    # \"wdir10\": \"wdir10_h\"                              \n",
    "                                    })\n",
    "    \n",
    "            # Subhourly data\n",
    "            ds_subh = xr.merge([\n",
    "                safe_xarray(H_subh, wind_search_string_subh),\n",
    "                # safe_xarray(H_subh, ':TMP:2 m|DPT')#.isel(step=[0,1,2])\n",
    "                safe_xarray(H_subh, 'GUST')\n",
    "            ], compat='minimal')\n",
    "    \n",
    "            ds_subh = ds_subh.sel(y=slicey, x=slicex).swap_dims({\"step\": \"valid_time\"})\n",
    "            \n",
    "            \n",
    "            # # get native data during this day - probably not needed for surface wind analysis\n",
    "            # H_nat = herbie.fast.FastHerbie(\n",
    "            #         time_range[:],\n",
    "            #         prioriy = 'google',\n",
    "            #         model=\"hrrr\",\n",
    "            #         product=\"nat\", # to get 15min steps backwards, set fxx to 1 and product \"subh\", otherwise \"sfc\", alternatively \"nat\"\n",
    "            #         fxx=[2]\n",
    "            #     )\n",
    "           \n",
    "            # #H_nat.inventory()[[\"variable\", \"search_this\"]].values \n",
    "                  \n",
    "            # # download seperate xarrays for different search strings \n",
    "            # ds1 = safe_xarray(H_nat, ':UGRD:1 hybrid level:2 hour fcst|:VGRD:1 hybrid level:2 hour fcst')\n",
    "            # ds1 = ds1.sel(y=slicey, x=slicex)  \n",
    "           \n",
    "            # ds_nat = xr.merge([ds1],compat='minimal')\n",
    "       \n",
    "            # # Add the time dimension\n",
    "            # ds_nat = ds_nat.expand_dims(dim=\"valid_time\")\n",
    "            \n",
    "    \n",
    "            # Merge hourly and subhourly datasets\n",
    "            hrrr = xr.merge([ds_sfc, ds_subh], compat='override')\n",
    "    \n",
    "            # Cleanup variables and attributes\n",
    "            hrrr = hrrr.drop_vars([\"boundaryLayerCloudLayer\", \"heightAboveGroundLayer\", \n",
    "                                   \"surface\", \"gribfile_projection\", \"time\"], errors=\"ignore\")\n",
    "            hrrr.attrs = {}\n",
    "            for var in hrrr.data_vars:\n",
    "                hrrr[var].attrs = {}\n",
    "    \n",
    "            return hrrr.chunk(chunk_dict_hourly)  # Apply chunking to hourly files\n",
    "    \n",
    "        except Exception as e:\n",
    "    \n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "            attempt += 1\n",
    "            print(f\"Attempt {attempt} failed. Retrying...\")\n",
    "            time.sleep(1)  # Wait before retrying        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d44731-8c9e-4166-85d0-a7755ad076b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-31 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-31.nc.\n",
      "2024-07-30 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-30.nc.\n",
      "2024-07-29 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-29.nc.\n",
      "2024-07-28 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-28.nc.\n",
      "2024-07-27 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-27.nc.\n",
      "2024-07-26 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-26.nc.\n",
      "2024-07-25 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-25.nc.\n",
      "2024-07-24 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-24.nc.\n",
      "2024-07-23 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-23.nc.\n",
      "2024-07-22 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-22.nc.\n",
      "2024-07-21 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-21.nc.\n",
      "2024-07-20 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-20.nc.\n",
      "2024-07-19 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-19.nc.\n",
      "2024-07-18 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-18.nc.\n",
      "2024-07-17 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-17.nc.\n",
      "2024-07-16 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-16.nc.\n",
      "2024-07-15 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-15.nc.\n",
      "2024-07-14 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-14.nc.\n",
      "2024-07-13 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-13.nc.\n",
      "2024-07-12 00:00:00\n",
      "File already exists: /kfs2/projects/sfcwinds/HRRR/hrrr_US_SW_2024-07-12.nc.\n",
      "2024-07-11 00:00:00\n",
      "2024-07-11 01:00:00\n",
      "2024-07-11 02:00:00\n",
      "2024-07-11 03:00:00\n",
      "2024-07-11 04:00:00\n",
      "2024-07-11 05:00:00\n",
      "Error: End of resource reached when reading message\n",
      "Attempt 1 failed. Retrying...\n",
      "2024-07-11 05:00:00\n",
      "Error: End of resource reached when reading message\n",
      "Attempt 2 failed. Retrying...\n",
      "2024-07-11 05:00:00\n",
      "Error: End of resource reached when reading message\n",
      "Attempt 3 failed. Retrying...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    t1 = datetime.now()\n",
    "    \n",
    "    # loop over days in date_range\n",
    "    for date in date_range[:]: \n",
    "        \n",
    "        print(date)\n",
    "        \n",
    "        daily_file_path = os.path.join(save_folder, f\"hrrr_{area}_{date.date()}.nc\")\n",
    "        \n",
    "        # Check if file already exists, exit loop if so\n",
    "        if os.path.exists(daily_file_path):\n",
    "            print(f\"File already exists: {daily_file_path}.\")\n",
    "            continue\n",
    "        \n",
    "        daily_datasets = []  # Store hourly datasets for the day\n",
    "        try:\n",
    "            del daily_hrrr\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        for hour in range(-fxx+1, fxx+21, 1):  # Looping to hours, so that valid_date goes from 0h to 24h\n",
    "            \n",
    "            timestamp = date + pd.to_timedelta(hour, \"h\")\n",
    "                 \n",
    "            # Load and process  hourly files      \n",
    "            hrrr = process_hourly_data(timestamp, area, slicey, slicex, fxx)\n",
    "            \n",
    "            if hrrr is not None:\n",
    "\n",
    "                # Store dataset in list\n",
    "                daily_datasets.append(hrrr)   \n",
    "                          \n",
    "                # # Save hourly file\n",
    "                # valid_time = timestamp + pd.to_timedelta(fxx, \"h\")\n",
    "                # hourly_file_path = os.path.join(save_folder, f\"hrrr_{area}_{valid_time.date()}_{valid_time.hour:02d}h.nc\")\n",
    "                # encoding = {\n",
    "                #     var: {**comp, \"chunksizes\": (1, 1, 200, 200)} # \n",
    "                #     if len(hrrr[var].dims) == 4 else {**comp, \"chunksizes\": (1, 200, 200)}\n",
    "                #     for var in hrrr.data_vars\n",
    "                #     }\n",
    "                # hrrr.to_netcdf(hourly_file_path, encoding=encoding) # '../data/HRRR\\\\hrrr_test.nc'\n",
    "                # # load: hrrr = xr.open_dataset(save_folder+\"/hrrr_NM_AZ_2024-12-31_00h.nc\", chunks=\"auto\")\n",
    "                # print (\"Saved hourly file.\")\n",
    "\n",
    "        # print (\"hourly files done\")\n",
    "        \n",
    "    \n",
    "        # Merge all hourly datasets into a daily file\n",
    "        daily_hrrr = xr.concat(daily_datasets, dim=\"valid_time\", combine_attrs=\"override\") \n",
    "        daily_hrrr = daily_hrrr.chunk(chunk_dict_daily)\n",
    "        \n",
    "            \n",
    "        # # save daily file\n",
    "        encoding = {\n",
    "            var: {**comp, \"chunksizes\": (12, 1, 200, 200)} # \n",
    "            if len(hrrr[var].dims) == 4 else {**comp, \"chunksizes\": (1, 200, 200)}\n",
    "            for var in hrrr.data_vars\n",
    "            }\n",
    "        daily_hrrr.to_netcdf(daily_file_path, encoding=encoding)\n",
    "        print (\"daily file saved\")\n",
    "        \n",
    "    t2 = datetime.now()\n",
    "    \n",
    "    print (t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3fc4c-25a7-4352-8b44-7af78547dc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa347334-5b7f-4ec4-a126-2aaa1d61667a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d40f9a4-d177-4f79-94d8-adcff033b067",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_hrrr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test hourly vs subourly wind speed\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(daily_hrrr\u001b[38;5;241m.\u001b[39mvalid_time\u001b[38;5;241m.\u001b[39mvalues, daily_hrrr\u001b[38;5;241m.\u001b[39misel(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mu10_h\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHRRR sfc\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(daily_hrrr\u001b[38;5;241m.\u001b[39mvalid_time\u001b[38;5;241m.\u001b[39mvalues, daily_hrrr\u001b[38;5;241m.\u001b[39misel(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mu10\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m\"\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHRRR subh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'daily_hrrr' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test hourly vs subourly wind speed\n",
    "plt.figure()\n",
    "plt.plot(daily_hrrr.valid_time.values, daily_hrrr.isel(x=100, y=100).u10_h.values, \".\", color=\"blue\", label = \"HRRR sfc\")    \n",
    "plt.plot(daily_hrrr.valid_time.values, daily_hrrr.isel(x=100, y=100).u10.values, \".\", color=\"orange\", label = \"HRRR subh\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aaa1008-7764-4d8b-8a26-6662cca5f51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:64: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:65: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:64: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:65: SyntaxWarning: invalid escape sequence '\\|'\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:64: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  ds4 = safe_xarray(H_sfc, '[U\\|V]GRD:10 m')\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:65: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  ds5 = safe_xarray(H_sfc, '[U\\|V]GRD:80 m')\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 00:00:00\n",
      "2024-12-31 01:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 02:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 03:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 04:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 05:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 06:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 07:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 08:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 09:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 10:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 11:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 12:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 13:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 14:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 15:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 16:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 17:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 18:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 19:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 20:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 21:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 22:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 23:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:27: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  timestamp = date + pd.to_timedelta(hour, \"H\")\n",
      "/tmp/scratch/7410880/ipykernel_555495/1114614423.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  valid_time = timestamp + pd.to_timedelta(fxx, \"H\")  # 2-hour shift (forecast lead time fxx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-01 00:00:00\n",
      "sfc file\n",
      "subh file\n",
      "merge\n",
      "hourly files done\n",
      "daily file saved\n"
     ]
    }
   ],
   "source": [
    "# old without functions    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # read HRRR in time range\n",
    "\n",
    "    # loop over days in date_range\n",
    "    for date in date_range[:1]: \n",
    "    \n",
    "        print(date)\n",
    "        \n",
    "        daily_file_path = os.path.join(save_folder, f\"hrrr_{area}_{date.date()}.nc\")\n",
    "        \n",
    "        # # Check if file already exists, exit loop if so\n",
    "        # if os.path.exists(daily_file_path):\n",
    "        #      print(f\"File already exists: {daily_file_path}.\")\n",
    "        #      continue\n",
    "        \n",
    "        daily_datasets = []  # Store hourly datasets for the day\n",
    "        try:\n",
    "            del daily_hrrr\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for hour in range(-1, 23, 1):  # Looping to cover 0h-22h, so valid_date goes up to 24h\n",
    "            \n",
    "            try:\n",
    "            \n",
    "                 timestamp = date + pd.to_timedelta(hour, \"h\") \n",
    "                 valid_time = timestamp + pd.to_timedelta(fxx, \"h\")  # 2-hour shift (forecast lead time fxx)\n",
    "        \n",
    "\n",
    "                \n",
    "                 print (valid_time)\n",
    "        \n",
    "                 # Define file path\n",
    "                 file_path = os.path.join(save_folder, f\"hrrr_{area}_{valid_time.date()}_{valid_time.hour:02d}h.nc\")\n",
    "                \n",
    "                 print(\"sfc file\")\n",
    "            \n",
    "                 # get hourly surface data during this day\n",
    "                 H_sfc = Herbie(\n",
    "                      timestamp,\n",
    "                     # priority = 'google',\n",
    "                      model='hrrr',\n",
    "                      product='sfc',\n",
    "                      fxx=fxx,\n",
    "                      verbose=False\n",
    "                    )\n",
    "                                    \n",
    "                 #H_sfc.file_exists   \n",
    "                 #H_sfc.inventory().variable.values   \n",
    "                 #H_sfc.inventory()[[\"variable\", \"search_this\"]].values    \n",
    "                 # H.xarray(\"(?:HGT|LAND):surface\")\n",
    "                       \n",
    "                 # download seperate xarrays for different search strings (why cannot search with one search string? - maybe different steps for each variable?)   \n",
    "                 df = H_sfc.inventory()[[\"variable\", \"search_this\"]]\n",
    "                 filtered_df = df[df[\"search_this\"].str.contains(r\"\\b(?:ugrd|vgrd)\\b\", case=False, na=False) & \n",
    "                                  df[\"search_this\"].str.contains(\"mb\", case=False, na=False)]\n",
    "                 wind_search_string = \"|\".join(filtered_df[\"search_this\"].drop_duplicates().values)\n",
    "                \n",
    "                 ds1 = safe_xarray(H_sfc, wind_search_string)\n",
    "                 ds2 = safe_xarray(H_sfc, ':TMP:2 m')\n",
    "                 ds3 = safe_xarray(H_sfc, 'GUST|:PRES:surface:2 hour fcst|:SNOWC:surface:2 hour fcst|:APCP:surface:0-2 hour acc fcst|:SFCR:surface:2 hour fcst|:VEG:surface:2 hour fcst|:VGTYP:surface:2 hour fcst|:HPBL:surface:2 hour fcst')\n",
    "                 ds4 = safe_xarray(H_sfc, '[U\\|V]GRD:10 m')\n",
    "                 ds5 = safe_xarray(H_sfc, '[U\\|V]GRD:80 m')\n",
    "                 ds5 = ds5.rename({\"u\": \"u80\",\n",
    "                                         \"v\": \"v80\",                              \n",
    "                                         })\n",
    "                 ds6 = safe_xarray(H_sfc, ':DPT:2 m above ground:2 hour fcst')\n",
    "                 ds7 = safe_xarray(H_sfc, ':TCDC:boundary layer cloud layer:2 hour fcst')\n",
    "                 ds8 = safe_xarray(H_sfc, ':CAPE:0-3000 m above ground:2 hour fcst')\n",
    "                \n",
    "        \n",
    "                 ds1 = ds1.sel(y=slicey, x=slicex)  \n",
    "                 ds2 = ds2.sel(y=slicey, x=slicex)  \n",
    "                 ds3 = ds3.sel(y=slicey, x=slicex) \n",
    "                 ds4 = ds4.sel(y=slicey, x=slicex)\n",
    "                 ds5 = ds5.sel(y=slicey, x=slicex)\n",
    "                 ds6 = ds6.sel(y=slicey, x=slicex) \n",
    "                 ds7 = ds7.sel(y=slicey, x=slicex)\n",
    "                 ds8 = ds8.sel(y=slicey, x=slicex)            \n",
    "        \n",
    "                \n",
    "                 ds_sfc = xr.merge([ds1,ds2, ds3, ds4, ds5, ds6, ds7, ds8],compat='minimal')\n",
    "                \n",
    "                 # ds_sfc['wspd10'] = (ds_sfc.u10**2 + ds_sfc.v10**2)**0.5\n",
    "                 # ds_sfc['wdir10'] = np.degrees(np.arctan2(ds_sfc.u10, ds_sfc.v10)) +180\n",
    "                \n",
    "                \n",
    "                 ds_sfc = ds_sfc.rename({\"u10\": \"u10_h\",\n",
    "                                         \"v10\": \"v10_h\",\n",
    "                                         # \"wspd10\": \"wspd10_h\",\n",
    "                                         # \"wdir10\": \"wdir10_h\"                              \n",
    "                                         })\n",
    "                \n",
    "                 # Add the time dimension\n",
    "                 ds_sfc = ds_sfc.expand_dims(dim=\"valid_time\")\n",
    "                \n",
    "                \n",
    "                 print(\"subh file\")\n",
    "                \n",
    "                 # get subhourly data during this day                \n",
    "                 H_subh = Herbie(\n",
    "                      timestamp,\n",
    "                      # priority = 'google',\n",
    "                      model='hrrr',\n",
    "                      product='subh',\n",
    "                      fxx=fxx,\n",
    "                      verbose=False\n",
    "                    )\n",
    "                \n",
    "                \n",
    "                 #H_subh.inventory()[[\"variable\", \"search_this\"]].values \n",
    "                \n",
    "                 # Extract wind variables\n",
    "                 df = H_subh.inventory()[[\"variable\", \"search_this\"]]\n",
    "                 filtered_df = df[df[\"search_this\"].str.contains(r\"\\b(?:ugrd|vgrd)\\b\", case=False, na=False) & \n",
    "                                  df[\"search_this\"].str.contains(\"ave\", case=False, na=False)]\n",
    "                 wind_search_string = \"|\".join(filtered_df[\"search_this\"].drop_duplicates().values)\n",
    "                \n",
    "           \n",
    "                 ds1 = safe_xarray(H_subh, wind_search_string)#.isel(step=[0,1,2]) # skip the last step because this is the forecast for the next hour\n",
    "                # ds2 = safe_xarray(H_subh, ':TMP:2 m|DPT')#.isel(step=[0,1,2])\n",
    "                 ds3 = safe_xarray(H_subh, 'GUST')#.isel(step=[0,1,2])\n",
    "                \n",
    "                 ds1 = ds1.sel(y=slicey, x=slicex)   \n",
    "                 #ds2 = ds2.sel(y=slicey, x=slicex)  \n",
    "                 ds3 = ds3.sel(y=slicey, x=slicex)   \n",
    "                \n",
    "                 ds_subh = xr.merge([ds1, ds3],compat='minimal')\n",
    "                \n",
    "                 # ds_subh['wspd10'] = (ds_subh.u10**2 + ds_subh.v10**2)**0.5\n",
    "                 # ds_subh['wdir10'] = np.degrees(np.arctan2(ds_subh.u10, ds_subh.v10)) +180\n",
    "            \n",
    "                 ds_subh = ds_subh.swap_dims({\"step\": \"valid_time\"})   \n",
    "                \n",
    "                    \n",
    "                 # # get native data during this day - probably not needed for surface wind analysis\n",
    "                 # H_nat = herbie.fast.FastHerbie(\n",
    "                 #         time_range[:],\n",
    "                 #         prioriy = 'google',\n",
    "                 #         model=\"hrrr\",\n",
    "                 #         product=\"nat\", # to get 15min steps backwards, set fxx to 1 and product \"subh\", otherwise \"sfc\", alternatively \"nat\"\n",
    "                 #         fxx=[2]\n",
    "                 #     )\n",
    "                \n",
    "                 # #H_nat.inventory()[[\"variable\", \"search_this\"]].values \n",
    "                       \n",
    "                 # # download seperate xarrays for different search strings \n",
    "                 # ds1 = safe_xarray(H_nat, ':UGRD:1 hybrid level:2 hour fcst|:VGRD:1 hybrid level:2 hour fcst')\n",
    "                 # ds1 = ds1.sel(y=slicey, x=slicex)  \n",
    "                \n",
    "                 # ds_nat = xr.merge([ds1],compat='minimal')\n",
    "            \n",
    "                 # # Add the time dimension\n",
    "                 # ds_nat = ds_nat.expand_dims(dim=\"valid_time\")\n",
    "            \n",
    "                 print(\"merge\")\n",
    "          \n",
    "                 # Merge hourly surface files and subhourly files\n",
    "                 hrrr = xr.merge([ds_sfc, ds_subh], compat='override')  # storage space is less when saving in combines datafile\n",
    "            \n",
    "                \n",
    "                 # Reduce file size (drop vars, chunking, compression)\n",
    "                 hrrr = hrrr.drop_vars([\"boundaryLayerCloudLayer\", \"heightAboveGroundLayer\", \"surface\", \"gribfile_projection\", \"time\"], errors=\"ignore\")\n",
    "                 hrrr.attrs = {}\n",
    "                 for var in hrrr.data_vars:\n",
    "                     hrrr[var].attrs = {}\n",
    "                \n",
    "                \n",
    "                 # Chunk hourly files\n",
    "                 hrrr = hrrr.chunk(chunks=\"auto\")\n",
    "                    \n",
    "                 # Store dataset in list\n",
    "                 daily_datasets.append(hrrr)\n",
    "                    \n",
    "                 # hrrr.to_netcdf(file_path, encoding=encoding) # '../data/HRRR\\\\hrrr_test.nc'\n",
    "                 # # load: hrrr = xr.open_dataset(save_folder+\"/hrrr_NM_AZ_2024-12-31_00h.nc\", chunks=\"auto\")\n",
    "                 # print (\"Saved hourly file.\")\n",
    "                        \n",
    "            except:\n",
    "                \n",
    "                attempt = 0\n",
    "                \n",
    "                while attempt < max_retries:\n",
    "                    attempt += 1\n",
    "                    print(f\"Attempt {attempt} failed. Retrying...\")\n",
    "                    time.sleep(1)  # Wait before retrying\n",
    "        \n",
    "        print (\"hourly files done\")\n",
    "        \n",
    "        chunk_dict = {\"valid_time\": 12, \"isobaricInhPa\": 1, \"y\": 200, \"x\": 200}\n",
    "        \n",
    "        # Apply consistent chunking to each dataset in daily_datasets\n",
    "        for i in range(len(daily_datasets)):\n",
    "            daily_datasets[i] = daily_datasets[i].chunk(chunk_dict)\n",
    "        \n",
    "        \n",
    "        # Merge all hourly datasets into a daily file\n",
    "        daily_hrrr = xr.concat(daily_datasets, dim=\"valid_time\", combine_attrs=\"override\") \n",
    "        daily_hrrr = daily_hrrr.chunk(chunk_dict)\n",
    "        \n",
    "            \n",
    "        # save daily file\n",
    "        comp = {\"zlib\": True, \"complevel\": 9}  # 1 being fastest, but lowest compression ratio, 9 being slowest but best compression ratio  \n",
    "        # encoding = {\n",
    "        #             var: {**comp, \"chunksizes\": (4, 2, 100, 100)} # \n",
    "        #             if len(hrrr[var].dims) == 4 else {**comp, \"chunksizes\": (4, 100, 100)}\n",
    "        #             for var in hrrr.data_vars\n",
    "        #         }\n",
    "        encoding = {var: {**comp, \"chunksizes\": None} for var in daily_hrrr.data_vars}\n",
    "        #daily_hrrr.to_netcdf(daily_file_path, encoding=encoding)\n",
    "        print (\"daily file saved\")\n",
    "           \n",
    "        # # Load the files:\n",
    "        # file_pattern = os.path.join(save_folder, f\"hrrr_{area}_{valid_time.date()}_*h.nc\")\n",
    "        # file_list = sorted(glob.glob(file_pattern))\n",
    "        # if file_list:\n",
    "        #     hrrr = xr.open_mfdataset(file_list[:], concat_dim =\"valid_time\",combine='nested', chunks=\"auto\", parallel = True)\n",
    "        \n",
    "        \n",
    "        # test hourly vs subourly wind speed\n",
    "        # plt.figure()\n",
    "        # plt.plot(daily_hrrr.valid_time.values, daily_hrrr.isel(x=100, y=100).u10_h.values, \".\", color=\"blue\", label = \"HRRR sfc\")    \n",
    "        # plt.plot(daily_hrrr.valid_time.values, daily_hrrr.isel(x=100, y=100).u10.values, \".\", color=\"orange\", label = \"HRRR subh\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55995a4b-1a8a-43f3-a17d-871808fe4832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:08.724218\n"
     ]
    }
   ],
   "source": [
    "# Test different nc save configurations\n",
    "\n",
    "test = daily_hrrr#.isel(valid_time=slice(0, 6))\n",
    "\n",
    "t1 = datetime.now()\n",
    "comp = {\"zlib\": True, \"complevel\": 4}  # 1 being fastest, but lowest compression ratio, 9 being slowest but best compression ratio  \n",
    "encoding = {\n",
    "            var: {**comp, \"chunksizes\": (12, 1, 200, 200)} # \n",
    "            if len(test[var].dims) == 4 else {**comp, \"chunksizes\": (12, 200, 200)}\n",
    "            for var in test.data_vars\n",
    "        }\n",
    "# encoding = {var: {**comp, \"chunksizes\": None} for var in daily_hrrr.data_vars}\n",
    "test.to_netcdf(os.path.join(save_folder, \"hrrr_test.nc\"), encoding=encoding)\n",
    "\n",
    "t2 = datetime.now()\n",
    "\n",
    "print (t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1953379e-594a-4214-8b3d-2ecb91353c2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingDimensionsError",
     "evalue": "'coordinates' has more than 1-dimension and the same name as one of its dimensions ('dim', 'coordinates'). xarray disallows such variables because they conflict with the coordinates used to label dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingDimensionsError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m         new_data_vars[var_name] \u001b[38;5;241m=\u001b[39m var_data\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Create a new xarray dataset with modified dimensions\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m new_ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m     59\u001b[0m     data_vars\u001b[38;5;241m=\u001b[39mnew_data_vars,\n\u001b[1;32m     60\u001b[0m     coords\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_time\u001b[39m\u001b[38;5;124m'\u001b[39m: ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_time\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Keep valid_time\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m'\u001b[39m: ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m'\u001b[39m), coordinates)  \u001b[38;5;66;03m# New 2-row coordinate variable\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     }\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Create a new HDF5 file to save the data\u001b[39;00m\n\u001b[1;32m     68\u001b[0m h5_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhrrr_test.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m~/.conda-envs/thrive/lib/python3.12/site-packages/xarray/core/dataset.py:652\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, data_vars, coords, attrs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(coords, Dataset):\n\u001b[1;32m    650\u001b[0m     coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39m_variables\n\u001b[0;32m--> 652\u001b[0m variables, coord_names, dims, indexes, _ \u001b[38;5;241m=\u001b[39m merge_data_and_coords(\n\u001b[1;32m    653\u001b[0m     data_vars, coords, compat\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbroadcast_equals\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    654\u001b[0m )\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs) \u001b[38;5;28;01mif\u001b[39;00m attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda-envs/thrive/lib/python3.12/site-packages/xarray/core/merge.py:566\u001b[0m, in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data_vars, coords, compat, join)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_data_and_coords\u001b[39m(\n\u001b[1;32m    560\u001b[0m     data_vars: Mapping[Any, Any],\n\u001b[1;32m    561\u001b[0m     coords: Mapping[Any, Any],\n\u001b[1;32m    562\u001b[0m     compat: CompatOptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbroadcast_equals\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    563\u001b[0m     join: JoinOptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    564\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _MergeResult:\n\u001b[1;32m    565\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Used in Dataset.__init__.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m     indexes, coords \u001b[38;5;241m=\u001b[39m _create_indexes_from_coords(coords, data_vars)\n\u001b[1;32m    567\u001b[0m     objects \u001b[38;5;241m=\u001b[39m [data_vars, coords]\n\u001b[1;32m    568\u001b[0m     explicit_coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/.conda-envs/thrive/lib/python3.12/site-packages/xarray/core/merge.py:602\u001b[0m, in \u001b[0;36m_create_indexes_from_coords\u001b[0;34m(coords, data_vars)\u001b[0m\n\u001b[1;32m    595\u001b[0m index_vars \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    596\u001b[0m     k: v\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m all_variables\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m coords \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, pd\u001b[38;5;241m.\u001b[39mMultiIndex)\n\u001b[1;32m    599\u001b[0m }\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m index_vars\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 602\u001b[0m     variable \u001b[38;5;241m=\u001b[39m as_variable(obj, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m==\u001b[39m (name,):\n\u001b[1;32m    605\u001b[0m         idx, idx_vars \u001b[38;5;241m=\u001b[39m create_default_index_implicit(variable, all_variables)\n",
      "File \u001b[0;32m~/.conda-envs/thrive/lib/python3.12/site-packages/xarray/core/variable.py:164\u001b[0m, in \u001b[0;36mas_variable\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mdims:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# convert the Variable into an Index\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MissingDimensionsError(\n\u001b[1;32m    165\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has more than 1-dimension and the same name as one of its \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. xarray disallows such variables because they \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflict with the coordinates used to label dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mto_index_variable()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mMissingDimensionsError\u001b[0m: 'coordinates' has more than 1-dimension and the same name as one of its dimensions ('dim', 'coordinates'). xarray disallows such variables because they conflict with the coordinates used to label dimensions."
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "hrrr = daily_hrrr\n",
    "\n",
    "# Replace u and v at pressure levels - does not save disk space\n",
    "hrrr['u_500hPa'] = hrrr['u'].sel(isobaricInhPa=500.0)  # Selecting u at 500 hPa\n",
    "hrrr['v_500hPa'] = hrrr['v'].sel(isobaricInhPa=500.0)  # Selecting v at 500 hPa\n",
    "\n",
    "hrrr['u_300hPa'] = hrrr['u'].sel(isobaricInhPa=300.0)  # Selecting u at 300 hPa\n",
    "hrrr['v_300hPa'] = hrrr['v'].sel(isobaricInhPa=300.0)  # Selecting v at 300 hPa\n",
    "\n",
    "hrrr['u_250hPa'] = hrrr['u'].sel(isobaricInhPa=250.0)  # Selecting u at 300 hPa\n",
    "hrrr['v_250hPa'] = hrrr['v'].sel(isobaricInhPa=250.0)  # Selecting v at 300 hPa\n",
    "\n",
    "hrrr['u_700hPa'] = hrrr['u'].sel(isobaricInhPa=700.0)  # Selecting u at 500 hPa\n",
    "hrrr['v_700hPa'] = hrrr['v'].sel(isobaricInhPa=700.0)  # Selecting v at 500 hPa\n",
    "\n",
    "hrrr['u_1000hPa'] = hrrr['u'].sel(isobaricInhPa=1000.0)  # Selecting u at 300 hPa\n",
    "hrrr['v_1000hPa'] = hrrr['v'].sel(isobaricInhPa=1000.0)  # Selecting v at 300 hPa\n",
    "\n",
    "hrrr['u_850hPa'] = hrrr['u'].sel(isobaricInhPa=850.0)  # Selecting u at 300 hPa\n",
    "hrrr['v_850hPa'] = hrrr['v'].sel(isobaricInhPa=850.0)  # Selecting v at 300 hPa\n",
    "\n",
    "hrrr['u_925hPa'] = hrrr['u'].sel(isobaricInhPa=925.0)  # Selecting u at 300 hPa\n",
    "hrrr['v_925hPa'] = hrrr['v'].sel(isobaricInhPa=925.0)  # Selecting v at 300 hPa\n",
    "\n",
    "\n",
    "# Step 3: Drop the original 'u' and 'v' variables\n",
    "hrrr = hrrr.drop_vars(['u', 'v'])\n",
    "hrrr = hrrr.drop_vars(['isobaricInhPa'])\n",
    "\n",
    "\n",
    "#### Reduce netcdf to valid_time - coordinates (coordinates = time * (x*y)) - this does not save any storage space in netcdf either,\n",
    "ds = hrrr\n",
    "\n",
    "# Get original spatial shape\n",
    "y_size, x_size = ds.sizes['y'], ds.sizes['x']\n",
    "num_points = y_size * x_size  # Total number of spatial points\n",
    "\n",
    "# Flatten latitude and longitude, then stack into a (2, x*y) shape\n",
    "lat_flat = ds['latitude'].values.ravel()  # Flatten to (x*y,)\n",
    "lon_flat = ds['longitude'].values.ravel()  # Flatten to (x*y,)\n",
    "coordinates = np.vstack([lat_flat, lon_flat])  # Shape (2, x*y)\n",
    "\n",
    "# Reshape all spatial variables to match new dimensions (valid_time, num_points)\n",
    "new_data_vars = {}\n",
    "for var_name, var_data in ds.data_vars.items():\n",
    "    if 'y' in var_data.dims and 'x' in var_data.dims:\n",
    "        # Flatten spatial dimensions while keeping valid_time\n",
    "        new_data_vars[var_name] = (('valid_time', 'coordinates'), var_data.values.reshape(len(ds['valid_time']), num_points))\n",
    "    elif 'valid_time' in var_data.dims:\n",
    "        # Keep variables that do not depend on (y, x) unchanged\n",
    "        new_data_vars[var_name] = var_data\n",
    "\n",
    "# Create a new xarray dataset with modified dimensions\n",
    "new_ds = xr.Dataset(\n",
    "    data_vars=new_data_vars,\n",
    "    coords={\n",
    "        'valid_time': ds['valid_time'],  # Keep valid_time\n",
    "        'coordinates': (('dim', 'coordinates'), coordinates)  # New 2-row coordinate variable\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Create a new HDF5 file to save the data\n",
    "h5_file = os.path.join(save_folder, f\"hrrr_test.h5\") \n",
    "\n",
    "with h5py.File(h5_file, 'w') as h5f:\n",
    "\n",
    "    # Save \"coordinates\" dataset (shape: 2, x*y) with compression\n",
    "    h5f.create_dataset('coordinates', data=new_ds['coordinates'].values, \n",
    "                        compression='gzip', compression_opts=9)\n",
    "    \n",
    "    # Save \"valid_time\" (convert datetime to string format for HDF5 compatibility)\n",
    "    valid_time_str = new_ds['valid_time'].astype(str).values  # Convert to string\n",
    "    h5f.create_dataset('valid_time', data=valid_time_str.astype('S'), \n",
    "                        compression='gzip', compression_opts=9)  # Store as bytes\n",
    "    \n",
    "    # Save all transformed data variables (valid_time, coordinates) with compression\n",
    "    for var_name, var_data in new_ds.data_vars.items():\n",
    "        h5f.create_dataset(var_name, data=var_data.values, \n",
    "                            compression='gzip', compression_opts=9)\n",
    "\n",
    "\n",
    "t2 = datetime.now()\n",
    "\n",
    "print (t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461eac35-afea-4241-841a-4a888baf6b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t2m': (('valid_time', 'coordinates'),\n",
       "  array([[     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         ...,\n",
       "         [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         [291.6185, 291.6185, 291.6185, ..., 274.431 , 274.3685, 274.3685]],\n",
       "        dtype=float32)),\n",
       " 'sp': (('valid_time', 'coordinates'),\n",
       "  array([[    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "         ...,\n",
       "         [    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "         [101760., 101760., 101760., ..., 101140., 101140., 101130.]],\n",
       "        dtype=float32)),\n",
       " 'blh': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [734.9012  , 749.6512  , 762.5262  , ...,  76.651184,  68.276184,\n",
       "           77.463684]], dtype=float32)),\n",
       " 'fsr': (('valid_time', 'coordinates'),\n",
       "  array([[   nan,    nan,    nan, ...,    nan,    nan,    nan],\n",
       "         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n",
       "         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n",
       "         ...,\n",
       "         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n",
       "         [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n",
       "         [0.e+00, 1.e-04, 1.e-04, ..., 0.e+00, 0.e+00, 0.e+00]],\n",
       "        dtype=float32)),\n",
       " 'tp': (('valid_time', 'coordinates'),\n",
       "  array([[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
       "         [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
       "         [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
       "         ...,\n",
       "         [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
       "         [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
       "         [0.   , 0.   , 0.001, ..., 0.   , 0.   , 0.   ]], dtype=float32)),\n",
       " 'snowc': (('valid_time', 'coordinates'),\n",
       "  array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)),\n",
       " 'gust': (('valid_time', 'coordinates'),\n",
       "  array([[      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         ...,\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [5.643394 , 5.705894 , 5.768394 , ..., 8.3933935, 8.7058935,\n",
       "          8.8933935]], dtype=float32)),\n",
       " 'veg': (('valid_time', 'coordinates'),\n",
       "  array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)),\n",
       " 'gppbfas': (('valid_time', 'coordinates'),\n",
       "  array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [17., 17., 17., ..., 17., 17., 17.]], dtype=float32)),\n",
       " 'u10_h': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-3.4041882, -3.5291882, -3.6541882, ...,  4.408312 ,  4.720812 ,\n",
       "           4.720812 ]], dtype=float32)),\n",
       " 'v10_h': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-4.2704735, -4.3329735, -4.3329735, ...,  3.9170265,  3.9170265,\n",
       "           3.9795265]], dtype=float32)),\n",
       " 'u80': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-3.5455437, -3.6080437, -3.7330437, ...,  7.5794563,  7.8294563,\n",
       "           8.079456 ]], dtype=float32)),\n",
       " 'v80': (('valid_time', 'coordinates'),\n",
       "  array([[      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         ...,\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [-4.350341, -4.412841, -4.412841, ...,  4.649659,  4.962159,\n",
       "           5.337159]], dtype=float32)),\n",
       " 'd2m': (('valid_time', 'coordinates'),\n",
       "  array([[     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         ...,\n",
       "         [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         [     nan,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "         [287.8123, 287.9373, 287.9998, ..., 274.4373, 274.3748, 274.3748]],\n",
       "        dtype=float32)),\n",
       " 'tcc': (('valid_time', 'coordinates'),\n",
       "  array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         ...,\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
       "         [ 63.,  66.,  76., ...,  92., 100., 100.]], dtype=float32)),\n",
       " 'cape': (('valid_time', 'coordinates'),\n",
       "  array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [nan, nan, nan, ..., nan, nan, nan],\n",
       "         [3.9, 4.8, 5.3, ..., 0. , 0. , 0. ]], dtype=float32)),\n",
       " 'u10': (('valid_time', 'coordinates'),\n",
       "  array([[-5.77, -5.82, -5.86, ..., -9.71, -9.64, -9.55],\n",
       "         [-6.02, -5.94, -5.86, ..., -9.74, -9.52, -9.26],\n",
       "         [-6.01, -6.02, -6.  , ..., -9.54, -9.38, -9.33],\n",
       "         ...,\n",
       "         [-3.46, -3.46, -3.53, ...,  4.09,  4.16,  4.07],\n",
       "         [-3.42, -3.5 , -3.61, ...,  4.4 ,  4.42,  4.35],\n",
       "         [-3.44, -3.53, -3.63, ...,  4.4 ,  4.67,  4.66]], dtype=float32)),\n",
       " 'v10': (('valid_time', 'coordinates'),\n",
       "  array([[-4.93, -4.98, -5.04, ...,  5.45,  5.41,  5.41],\n",
       "         [-4.96, -4.98, -4.98, ...,  6.12,  6.08,  6.06],\n",
       "         [-4.85, -4.95, -4.97, ...,  6.87,  6.97,  7.16],\n",
       "         ...,\n",
       "         [-5.25, -5.24, -5.26, ...,  5.54,  5.47,  5.35],\n",
       "         [-4.67, -4.7 , -4.76, ...,  4.88,  4.85,  4.79],\n",
       "         [-4.29, -4.33, -4.37, ...,  4.09,  4.1 ,  4.1 ]], dtype=float32)),\n",
       " 'u_500hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-0.9860611, -0.9860611, -0.9860611, ..., 16.326439 , 16.263939 ,\n",
       "          16.138939 ]], dtype=float32)),\n",
       " 'v_500hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [ 5.6482506,  5.6482506,  5.6482506, ..., 11.023251 , 11.085751 ,\n",
       "          11.148251 ]], dtype=float32)),\n",
       " 'u_300hPa': (('valid_time', 'coordinates'),\n",
       "  array([[      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         ...,\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [ 8.441592,  8.441592,  8.441592, ..., 16.379093, 16.379093,\n",
       "          16.316593]], dtype=float32)),\n",
       " 'v_300hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-2.9421082, -2.9421082, -2.9421082, ..., 13.745392 , 13.932892 ,\n",
       "          14.120392 ]], dtype=float32)),\n",
       " 'u_250hPa': (('valid_time', 'coordinates'),\n",
       "  array([[      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         ...,\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [ 8.896365,  8.958865,  8.958865, ..., 13.146365, 13.021365,\n",
       "          12.896365]], dtype=float32)),\n",
       " 'v_250hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-4.3525543, -4.3525543, -4.2900543, ..., 21.022446 , 21.022446 ,\n",
       "          21.022446 ]], dtype=float32)),\n",
       " 'u_700hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-5.2236767, -5.2236767, -5.2236767, ..., 10.776323 , 10.776323 ,\n",
       "          10.713823 ]], dtype=float32)),\n",
       " 'v_700hPa': (('valid_time', 'coordinates'),\n",
       "  array([[        nan,         nan,         nan, ...,         nan,\n",
       "                  nan,         nan],\n",
       "         [        nan,         nan,         nan, ...,         nan,\n",
       "                  nan,         nan],\n",
       "         [        nan,         nan,         nan, ...,         nan,\n",
       "                  nan,         nan],\n",
       "         ...,\n",
       "         [        nan,         nan,         nan, ...,         nan,\n",
       "                  nan,         nan],\n",
       "         [        nan,         nan,         nan, ...,         nan,\n",
       "                  nan,         nan],\n",
       "         [ 0.46255875,  0.46255875,  0.46255875, ..., 10.462559  ,\n",
       "          10.462559  , 10.462559  ]], dtype=float32)),\n",
       " 'u_1000hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-3.6012259, -3.6012259, -3.6012259, ...,  7.773774 ,  7.898774 ,\n",
       "           8.023774 ]], dtype=float32)),\n",
       " 'v_1000hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-4.4664392, -4.4664392, -4.5289392, ...,  4.5960608,  4.7835608,\n",
       "           5.0335608]], dtype=float32)),\n",
       " 'u_850hPa': (('valid_time', 'coordinates'),\n",
       "  array([[      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         ...,\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [      nan,       nan,       nan, ...,       nan,       nan,\n",
       "                nan],\n",
       "         [0.4178028, 0.4803028, 0.4803028, ..., 4.730303 , 4.730303 ,\n",
       "          4.730303 ]], dtype=float32)),\n",
       " 'v_850hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-2.9049282, -2.7799282, -2.7174282, ...,  8.220072 ,  8.345072 ,\n",
       "           8.407572 ]], dtype=float32)),\n",
       " 'u_925hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-2.1081839, -2.1081839, -2.1081839, ...,  4.704316 ,  4.641816 ,\n",
       "           4.579316 ]], dtype=float32)),\n",
       " 'v_925hPa': (('valid_time', 'coordinates'),\n",
       "  array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [-2.1915035, -2.2540035, -2.3165035, ...,  5.4959965,  5.4959965,\n",
       "           5.4959965]], dtype=float32))}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab723ff-f37b-46c9-b99c-3e0fb36e153d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thrive",
   "language": "python",
   "name": "thrive"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
